{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dbe583f",
   "metadata": {
    "papermill": {
     "duration": 0.01016,
     "end_time": "2024-07-06T22:46:31.529307",
     "exception": false,
     "start_time": "2024-07-06T22:46:31.519147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc4bea0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:31.550422Z",
     "iopub.status.busy": "2024-07-06T22:46:31.549578Z",
     "iopub.status.idle": "2024-07-06T22:46:31.657384Z",
     "shell.execute_reply": "2024-07-06T22:46:31.656655Z"
    },
    "papermill": {
     "duration": 0.120443,
     "end_time": "2024-07-06T22:46:31.659527",
     "exception": false,
     "start_time": "2024-07-06T22:46:31.539084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b6a642d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:31.680651Z",
     "iopub.status.busy": "2024-07-06T22:46:31.680344Z",
     "iopub.status.idle": "2024-07-06T22:46:39.407656Z",
     "shell.execute_reply": "2024-07-06T22:46:39.406673Z"
    },
    "papermill": {
     "duration": 7.74048,
     "end_time": "2024-07-06T22:46:39.410053",
     "exception": false,
     "start_time": "2024-07-06T22:46:31.669573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "\n",
    "# Import torchvision \n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, models\n",
    "import albumentations\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f0f0be",
   "metadata": {
    "papermill": {
     "duration": 0.00925,
     "end_time": "2024-07-06T22:46:39.429218",
     "exception": false,
     "start_time": "2024-07-06T22:46:39.419968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SET UP Device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac91ef0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:39.449409Z",
     "iopub.status.busy": "2024-07-06T22:46:39.449001Z",
     "iopub.status.idle": "2024-07-06T22:46:39.511640Z",
     "shell.execute_reply": "2024-07-06T22:46:39.510674Z"
    },
    "papermill": {
     "duration": 0.075074,
     "end_time": "2024-07-06T22:46:39.513778",
     "exception": false,
     "start_time": "2024-07-06T22:46:39.438704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# device dog shit code \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a543f17",
   "metadata": {
    "papermill": {
     "duration": 0.009449,
     "end_time": "2024-07-06T22:46:39.532981",
     "exception": false,
     "start_time": "2024-07-06T22:46:39.523532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading the DaTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36c0f76a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:39.554212Z",
     "iopub.status.busy": "2024-07-06T22:46:39.553919Z",
     "iopub.status.idle": "2024-07-06T22:46:39.563818Z",
     "shell.execute_reply": "2024-07-06T22:46:39.562983Z"
    },
    "papermill": {
     "duration": 0.022357,
     "end_time": "2024-07-06T22:46:39.565710",
     "exception": false,
     "start_time": "2024-07-06T22:46:39.543353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Cassava Bacterial Blight (CBB)',\n",
       " 1: 'Cassava Brown Streak Disease (CBSD)',\n",
       " 2: 'Cassava Green Mottle (CGM)',\n",
       " 3: 'Cassava Mosaic Disease (CMD)',\n",
       " 4: 'Healthy'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data loading in \n",
    "\n",
    "base_path = '/kaggle/input/cassava-leaf-disease-classification/'\n",
    "\n",
    "train_path = '/kaggle/input/cassava-leaf-disease-classification/train_images'\n",
    "\n",
    "test_path = '/kaggle/input/cassava-leaf-disease-classification/test_images'\n",
    "\n",
    "\n",
    "with open(base_path+'label_num_to_disease_map.json') as f :\n",
    "    mapping = json.loads(f.read())\n",
    "    mapping = {int(k): v for k, v in mapping.items()}\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3c808b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:39.587282Z",
     "iopub.status.busy": "2024-07-06T22:46:39.587016Z",
     "iopub.status.idle": "2024-07-06T22:46:39.629053Z",
     "shell.execute_reply": "2024-07-06T22:46:39.628248Z"
    },
    "papermill": {
     "duration": 0.055542,
     "end_time": "2024-07-06T22:46:39.631034",
     "exception": false,
     "start_time": "2024-07-06T22:46:39.575492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21392</th>\n",
       "      <td>999068805.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21393</th>\n",
       "      <td>999329392.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21394</th>\n",
       "      <td>999474432.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21395</th>\n",
       "      <td>999616605.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21396</th>\n",
       "      <td>999998473.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_id  label\n",
       "21392  999068805.jpg      3\n",
       "21393  999329392.jpg      3\n",
       "21394  999474432.jpg      1\n",
       "21395  999616605.jpg      4\n",
       "21396  999998473.jpg      4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/cassava-leaf-disease-classification/train.csv')\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91aa541c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:39.652302Z",
     "iopub.status.busy": "2024-07-06T22:46:39.652023Z",
     "iopub.status.idle": "2024-07-06T22:46:39.658991Z",
     "shell.execute_reply": "2024-07-06T22:46:39.658067Z"
    },
    "papermill": {
     "duration": 0.019901,
     "end_time": "2024-07-06T22:46:39.661048",
     "exception": false,
     "start_time": "2024-07-06T22:46:39.641147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset responsible for manipulating data for training as well as training tests.\n",
    "class DatasetLeaf(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        super().__init__()\n",
    "        self.data = data.reset_index(drop=True).copy()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data.iloc[index]\n",
    "                \n",
    "        image = Image.open(f'/kaggle/input/cassava-leaf-disease-classification/train_images/{item[\"image_id\"]}')\n",
    "        label = item['label']\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c29f30d",
   "metadata": {
    "papermill": {
     "duration": 0.009689,
     "end_time": "2024-07-06T22:46:39.680694",
     "exception": false,
     "start_time": "2024-07-06T22:46:39.671005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CONFIG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dbd8c45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:39.702430Z",
     "iopub.status.busy": "2024-07-06T22:46:39.701624Z",
     "iopub.status.idle": "2024-07-06T22:46:39.706060Z",
     "shell.execute_reply": "2024-07-06T22:46:39.705334Z"
    },
    "papermill": {
     "duration": 0.017097,
     "end_time": "2024-07-06T22:46:39.707859",
     "exception": false,
     "start_time": "2024-07-06T22:46:39.690762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 12\n",
    "VALID_SIZE = 0.15 # percentage of data for validation .\n",
    "DIM = (256, 256)\n",
    "Width, Height = DIM\n",
    "epochs = 12\n",
    "NUM_Classes = 5\n",
    "NUM_Workers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1845f815",
   "metadata": {
    "papermill": {
     "duration": 0.009886,
     "end_time": "2024-07-06T22:46:39.728004",
     "exception": false,
     "start_time": "2024-07-06T22:46:39.718118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Creating Augmntations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5194a0ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:39.749312Z",
     "iopub.status.busy": "2024-07-06T22:46:39.749040Z",
     "iopub.status.idle": "2024-07-06T22:46:39.765091Z",
     "shell.execute_reply": "2024-07-06T22:46:39.764083Z"
    },
    "papermill": {
     "duration": 0.028816,
     "end_time": "2024-07-06T22:46:39.766908",
     "exception": false,
     "start_time": "2024-07-06T22:46:39.738092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train: 18188\n",
      "Length valid: 3209\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform_train = transforms.Compose([\n",
    "   # transforms.RandomRotation(0, 0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=DIM),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Normalize(mean=[0.4303, 0.4967, 0.3134],\n",
    "                std=[0.2330, 0.2359, 0.2237]),\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=DIM),\n",
    "    transforms.Normalize(mean=[0.4303, 0.4967, 0.3134],\n",
    "                std=[0.2330, 0.2359, 0.2237]),\n",
    "])\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=DIM),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creating datasets for training and validation\n",
    "train_data = DatasetLeaf(train, transform_train)\n",
    "valid_data = DatasetLeaf(train, transform_valid)\n",
    "\n",
    "\n",
    "# Shuffling data and choosing data that will be used for training and validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(VALID_SIZE * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, num_workers=NUM_Workers, sampler=train_sampler, pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=BATCH_SIZE, num_workers=NUM_Workers, sampler=valid_sampler, pin_memory=True)\n",
    "\n",
    "print(f\"Length train: {len(train_idx)}\")\n",
    "print(f\"Length valid: {len(valid_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa9cfa",
   "metadata": {
    "papermill": {
     "duration": 0.009727,
     "end_time": "2024-07-06T22:46:39.786751",
     "exception": false,
     "start_time": "2024-07-06T22:46:39.777024",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Some Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a8d9fcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:39.808716Z",
     "iopub.status.busy": "2024-07-06T22:46:39.807930Z",
     "iopub.status.idle": "2024-07-06T22:46:39.821098Z",
     "shell.execute_reply": "2024-07-06T22:46:39.820170Z"
    },
    "papermill": {
     "duration": 0.026353,
     "end_time": "2024-07-06T22:46:39.823155",
     "exception": false,
     "start_time": "2024-07-06T22:46:39.796802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               scheduler,\n",
    "               lrs,\n",
    "               device: torch.device = device):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    model.to(device)\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # Send data to GPU\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy_fn(y_true=y,\n",
    "                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        lrs.append(get_lr(optimizer))\n",
    "        scheduler.step()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate loss and accuracy per epoch and print out what's happening\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
    "    \n",
    "    return train_loss.cpu(), train_acc\n",
    "    \n",
    "\n",
    "def test_step(data_loader: torch.utils.data.DataLoader,\n",
    "              model: torch.nn.Module,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy_fn,\n",
    "              device: torch.device = device):\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.to(device)\n",
    "    model.eval() # put model in eval mode\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode(): \n",
    "        for X, y in data_loader:\n",
    "            # Send data to GPU\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X)\n",
    "            \n",
    "            # 2. Calculate loss and accuracy\n",
    "            test_loss += loss_fn(test_pred, y)\n",
    "            test_acc += accuracy_fn(y_true=y,\n",
    "                y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
    "            )\n",
    "        \n",
    "        # Adjust metrics and print out\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "        \n",
    "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")\n",
    "        return test_loss.cpu(), test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900f965b",
   "metadata": {
    "papermill": {
     "duration": 0.00967,
     "end_time": "2024-07-06T22:46:39.843168",
     "exception": false,
     "start_time": "2024-07-06T22:46:39.833498",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# pre Trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3305b562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:39.864276Z",
     "iopub.status.busy": "2024-07-06T22:46:39.863991Z",
     "iopub.status.idle": "2024-07-06T22:46:39.870163Z",
     "shell.execute_reply": "2024-07-06T22:46:39.869295Z"
    },
    "papermill": {
     "duration": 0.019161,
     "end_time": "2024-07-06T22:46:39.872207",
     "exception": false,
     "start_time": "2024-07-06T22:46:39.853046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_model():\\n    net = models.resnet152(weights=True)\\n    for param in net.parameters():\\n        param.requires_grad = True\\n        \\n    num_ft = net.fc.in_features\\n    net.fc = nn.Sequential(\\n                    nn.Linear(num_ft, 256),\\n                    nn.ReLU(),\\n                    nn.Dropout(0.2),\\n                    nn.Linear(256, NUM_Classes),\\n                    nn.LogSoftmax(dim=1)\\n    )\\n    \\n    net.to(device)\\n    return net \\n    \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def get_model():\n",
    "    net = models.resnet152(weights=True)\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    num_ft = net.fc.in_features\n",
    "    net.fc = nn.Sequential(\n",
    "                    nn.Linear(num_ft, 256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.Linear(256, NUM_Classes),\n",
    "                    nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "    \n",
    "    net.to(device)\n",
    "    return net \n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bd120d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:39.893502Z",
     "iopub.status.busy": "2024-07-06T22:46:39.893257Z",
     "iopub.status.idle": "2024-07-06T22:46:39.896712Z",
     "shell.execute_reply": "2024-07-06T22:46:39.895867Z"
    },
    "papermill": {
     "duration": 0.016217,
     "end_time": "2024-07-06T22:46:39.898608",
     "exception": false,
     "start_time": "2024-07-06T22:46:39.882391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f0fdf20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:39.919896Z",
     "iopub.status.busy": "2024-07-06T22:46:39.919603Z",
     "iopub.status.idle": "2024-07-06T22:46:39.923224Z",
     "shell.execute_reply": "2024-07-06T22:46:39.922428Z"
    },
    "papermill": {
     "duration": 0.016563,
     "end_time": "2024-07-06T22:46:39.925170",
     "exception": false,
     "start_time": "2024-07-06T22:46:39.908607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# layers = list(range(8))\n",
    "# i = 0\n",
    "# for layer in model.children():\n",
    "#     if i in layers :\n",
    "#         for param in layer.parameters():\n",
    "#             param.requires_grad = True\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e13b6c05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:39.946714Z",
     "iopub.status.busy": "2024-07-06T22:46:39.946394Z",
     "iopub.status.idle": "2024-07-06T22:46:39.950461Z",
     "shell.execute_reply": "2024-07-06T22:46:39.949651Z"
    },
    "papermill": {
     "duration": 0.01756,
     "end_time": "2024-07-06T22:46:39.952812",
     "exception": false,
     "start_time": "2024-07-06T22:46:39.935252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey\n"
     ]
    }
   ],
   "source": [
    "print('hey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ee879b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:39.974399Z",
     "iopub.status.busy": "2024-07-06T22:46:39.974140Z",
     "iopub.status.idle": "2024-07-06T22:46:42.465964Z",
     "shell.execute_reply": "2024-07-06T22:46:42.464991Z"
    },
    "papermill": {
     "duration": 2.505093,
     "end_time": "2024-07-06T22:46:42.468275",
     "exception": false,
     "start_time": "2024-07-06T22:46:39.963182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.load('/kaggle/input/resnn/pytorch/ressnt/1/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f161ae6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:42.491439Z",
     "iopub.status.busy": "2024-07-06T22:46:42.491094Z",
     "iopub.status.idle": "2024-07-06T22:46:42.501376Z",
     "shell.execute_reply": "2024-07-06T22:46:42.500500Z"
    },
    "papermill": {
     "duration": 0.024015,
     "end_time": "2024-07-06T22:46:42.503845",
     "exception": false,
     "start_time": "2024-07-06T22:46:42.479830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58,669,637 total parameters\n",
      "58,669,637 training parameters\n"
     ]
    }
   ],
   "source": [
    " # find total parameters and trainable parameters\n",
    " total_params = sum(p.numel() for p in model.parameters())\n",
    " print(f'{total_params:,} total parameters')\n",
    " trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    " print(f'{trainable_params:,} training parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2433e8b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:42.525495Z",
     "iopub.status.busy": "2024-07-06T22:46:42.525237Z",
     "iopub.status.idle": "2024-07-06T22:46:42.532262Z",
     "shell.execute_reply": "2024-07-06T22:46:42.531337Z"
    },
    "papermill": {
     "duration": 0.019985,
     "end_time": "2024-07-06T22:46:42.534223",
     "exception": false,
     "start_time": "2024-07-06T22:46:42.514238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport requests\\nfrom pathlib import Path \\n\\n# Download helper functions from Learn PyTorch repo (if not already downloaded)\\nif Path(\"helper_functions.py\").is_file():\\n  print(\"helper_functions.py already exists, skipping download\")\\nelse:\\n  print(\"Downloading helper_functions.py\")\\n  # Note: you need the \"raw\" GitHub URL for this to work\\n  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\\n  with open(\"helper_functions.py\", \"wb\") as f:\\n        \\n    f.write(request.content)\\n    \\n    \\n# Import accuracy metric\\nfrom helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = \\'multiclass\\', num_classes=len(class_names)).to(device)\\n\\n# Setup loss function and optimizer\\n\\nloss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\\noptimizer = torch.optim.SGD(params=model.parameters(), lr=0.001, momentum=0.9)\\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,max_lr=0.01, steps_per_epoch=len(train_loader), epochs=5)\\n\\n\\ndef get_lr(optimizer):\\n    for param_group in optimizer.param_groups:\\n        return param_group[\\'lr\\']\\n\\n\\nfrom timeit import default_timer as timer \\ndef print_train_time(start: float, end: float, device: torch.device = None):\\n\\n    total_time = end - start\\n    print(f\"Train time on {device}: {total_time:.3f} seconds\")\\n    return total_time\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  # Note: you need the \"raw\" GitHub URL for this to work\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "        \n",
    "    f.write(request.content)\n",
    "    \n",
    "    \n",
    "# Import accuracy metric\n",
    "from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,max_lr=0.01, steps_per_epoch=len(train_loader), epochs=5)\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "103190b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:42.556173Z",
     "iopub.status.busy": "2024-07-06T22:46:42.555928Z",
     "iopub.status.idle": "2024-07-06T22:46:42.559670Z",
     "shell.execute_reply": "2024-07-06T22:46:42.558847Z"
    },
    "papermill": {
     "duration": 0.016824,
     "end_time": "2024-07-06T22:46:42.561559",
     "exception": false,
     "start_time": "2024-07-06T22:46:42.544735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d233d4a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:42.583472Z",
     "iopub.status.busy": "2024-07-06T22:46:42.583235Z",
     "iopub.status.idle": "2024-07-06T22:46:42.589838Z",
     "shell.execute_reply": "2024-07-06T22:46:42.589018Z"
    },
    "papermill": {
     "duration": 0.020383,
     "end_time": "2024-07-06T22:46:42.592381",
     "exception": false,
     "start_time": "2024-07-06T22:46:42.571998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.manual_seed(42)\\n\\n# Measure time\\nfrom timeit import default_timer as timer\\ntrain_time_start_on_gpu = timer()\\n\\nepochs = 5\\n\\n # 2. Create empty results dictionary\\nresults = {\"train_loss\": [],\\n        \"train_acc\": [],\\n        \"test_loss\": [],\\n        \"test_acc\": []\\n    }\\n\\nfor epoch in tqdm(range(epochs)):\\n    print(f\"Epoch: {epoch}\\n---------\")\\n    lrs = []\\n    train_loss, train_acc = train_step(data_loader=train_loader, \\n        model=model, \\n        loss_fn=loss_fn,\\n        optimizer=optimizer,\\n        accuracy_fn=accuracy_fn,\\n        scheduler=scheduler,\\n        lrs=lrs,\\n    \\n    )\\n    test_loss, test_acc = test_step(data_loader=valid_loader,\\n        model=model,\\n        loss_fn=loss_fn,\\n        accuracy_fn=accuracy_fn\\n    )\\n    \\n    # 5. Update results dictionary\\n    results[\"train_loss\"].append(train_loss.detach().numpy())\\n    results[\"train_acc\"].append(train_acc)\\n    results[\"test_loss\"].append(test_loss.detach().numpy())\\n    results[\"test_acc\"].append(test_acc)\\n\\n\\ntrain_time_end_on_gpu = timer()\\ntotal_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\\n                                            end=train_time_end_on_gpu,\\n                                            device=device)\\n                                            \\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Measure time\n",
    "from timeit import default_timer as timer\n",
    "train_time_start_on_gpu = timer()\n",
    "\n",
    "epochs = 5\n",
    "\n",
    " # 2. Create empty results dictionary\n",
    "results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    lrs = []\n",
    "    train_loss, train_acc = train_step(data_loader=train_loader, \n",
    "        model=model, \n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn,\n",
    "        scheduler=scheduler,\n",
    "        lrs=lrs,\n",
    "    \n",
    "    )\n",
    "    test_loss, test_acc = test_step(data_loader=valid_loader,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )\n",
    "    \n",
    "    # 5. Update results dictionary\n",
    "    results[\"train_loss\"].append(train_loss.detach().numpy())\n",
    "    results[\"train_acc\"].append(train_acc)\n",
    "    results[\"test_loss\"].append(test_loss.detach().numpy())\n",
    "    results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "\n",
    "train_time_end_on_gpu = timer()\n",
    "total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n",
    "                                            end=train_time_end_on_gpu,\n",
    "                                            device=device)\n",
    "                                            \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f44645e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:42.614936Z",
     "iopub.status.busy": "2024-07-06T22:46:42.614695Z",
     "iopub.status.idle": "2024-07-06T22:46:42.622187Z",
     "shell.execute_reply": "2024-07-06T22:46:42.621280Z"
    },
    "papermill": {
     "duration": 0.020871,
     "end_time": "2024-07-06T22:46:42.624314",
     "exception": false,
     "start_time": "2024-07-06T22:46:42.603443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_loss_curves(results):\n",
    "    \n",
    "    # Get the loss values of the results dictionary (training and test)\n",
    "    loss = results['train_loss']\n",
    "    test_loss = results['test_loss']\n",
    "\n",
    "    # Get the accuracy values of the results dictionary (training and test)\n",
    "    accuracy = results['train_acc']\n",
    "    test_accuracy = results['test_acc']\n",
    "\n",
    "    # Figure out how many epochs there were\n",
    "    epochs = range(len(results['train_loss']))\n",
    "\n",
    "    # Setup a plot \n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label='train_loss')\n",
    "    plt.plot(epochs, test_loss, label='test_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
    "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df0a723e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:42.646466Z",
     "iopub.status.busy": "2024-07-06T22:46:42.646223Z",
     "iopub.status.idle": "2024-07-06T22:46:42.649829Z",
     "shell.execute_reply": "2024-07-06T22:46:42.649017Z"
    },
    "papermill": {
     "duration": 0.016801,
     "end_time": "2024-07-06T22:46:42.651804",
     "exception": false,
     "start_time": "2024-07-06T22:46:42.635003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot_loss_curves(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e707364",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:42.673945Z",
     "iopub.status.busy": "2024-07-06T22:46:42.673700Z",
     "iopub.status.idle": "2024-07-06T22:46:42.679493Z",
     "shell.execute_reply": "2024-07-06T22:46:42.678571Z"
    },
    "papermill": {
     "duration": 0.018857,
     "end_time": "2024-07-06T22:46:42.681366",
     "exception": false,
     "start_time": "2024-07-06T22:46:42.662509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nresults = {\"train_loss\": [],\\n        \"train_acc\": [],\\n        \"test_loss\": [],\\n        \"test_acc\": []\\n    }\\n\\nfor i in range(1):\\n    test_loss, test_acc = test_step(data_loader=valid_loader,\\n            model=model,\\n            loss_fn=loss_fn,\\n            accuracy_fn=accuracy_fn\\n        )\\n\\n# 5. Update results dictionary\\n    results[\"test_loss\"].append(test_loss.detach().numpy())\\n    results[\"test_acc\"].append(test_acc)\\n    '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "for i in range(1):\n",
    "    test_loss, test_acc = test_step(data_loader=valid_loader,\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            accuracy_fn=accuracy_fn\n",
    "        )\n",
    "\n",
    "# 5. Update results dictionary\n",
    "    results[\"test_loss\"].append(test_loss.detach().numpy())\n",
    "    results[\"test_acc\"].append(test_acc)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9d731be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:42.703836Z",
     "iopub.status.busy": "2024-07-06T22:46:42.703530Z",
     "iopub.status.idle": "2024-07-06T22:46:42.707030Z",
     "shell.execute_reply": "2024-07-06T22:46:42.706218Z"
    },
    "papermill": {
     "duration": 0.016807,
     "end_time": "2024-07-06T22:46:42.708895",
     "exception": false,
     "start_time": "2024-07-06T22:46:42.692088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#torch.save(model, 'model.fpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9440f991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:42.731172Z",
     "iopub.status.busy": "2024-07-06T22:46:42.730904Z",
     "iopub.status.idle": "2024-07-06T22:46:47.122567Z",
     "shell.execute_reply": "2024-07-06T22:46:47.121317Z"
    },
    "papermill": {
     "duration": 4.405104,
     "end_time": "2024-07-06T22:46:47.124761",
     "exception": false,
     "start_time": "2024-07-06T22:46:42.719657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([64, 3, 7, 7])\n",
      "bn1.weight \t torch.Size([64])\n",
      "bn1.bias \t torch.Size([64])\n",
      "bn1.running_mean \t torch.Size([64])\n",
      "bn1.running_var \t torch.Size([64])\n",
      "bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.conv1.weight \t torch.Size([64, 64, 1, 1])\n",
      "layer1.0.bn1.weight \t torch.Size([64])\n",
      "layer1.0.bn1.bias \t torch.Size([64])\n",
      "layer1.0.bn1.running_mean \t torch.Size([64])\n",
      "layer1.0.bn1.running_var \t torch.Size([64])\n",
      "layer1.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.weight \t torch.Size([64])\n",
      "layer1.0.bn2.bias \t torch.Size([64])\n",
      "layer1.0.bn2.running_mean \t torch.Size([64])\n",
      "layer1.0.bn2.running_var \t torch.Size([64])\n",
      "layer1.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.conv3.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.0.bn3.weight \t torch.Size([256])\n",
      "layer1.0.bn3.bias \t torch.Size([256])\n",
      "layer1.0.bn3.running_mean \t torch.Size([256])\n",
      "layer1.0.bn3.running_var \t torch.Size([256])\n",
      "layer1.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.downsample.0.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.0.downsample.1.weight \t torch.Size([256])\n",
      "layer1.0.downsample.1.bias \t torch.Size([256])\n",
      "layer1.0.downsample.1.running_mean \t torch.Size([256])\n",
      "layer1.0.downsample.1.running_var \t torch.Size([256])\n",
      "layer1.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer1.1.conv1.weight \t torch.Size([64, 256, 1, 1])\n",
      "layer1.1.bn1.weight \t torch.Size([64])\n",
      "layer1.1.bn1.bias \t torch.Size([64])\n",
      "layer1.1.bn1.running_mean \t torch.Size([64])\n",
      "layer1.1.bn1.running_var \t torch.Size([64])\n",
      "layer1.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.weight \t torch.Size([64])\n",
      "layer1.1.bn2.bias \t torch.Size([64])\n",
      "layer1.1.bn2.running_mean \t torch.Size([64])\n",
      "layer1.1.bn2.running_var \t torch.Size([64])\n",
      "layer1.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer1.1.conv3.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.1.bn3.weight \t torch.Size([256])\n",
      "layer1.1.bn3.bias \t torch.Size([256])\n",
      "layer1.1.bn3.running_mean \t torch.Size([256])\n",
      "layer1.1.bn3.running_var \t torch.Size([256])\n",
      "layer1.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer1.2.conv1.weight \t torch.Size([64, 256, 1, 1])\n",
      "layer1.2.bn1.weight \t torch.Size([64])\n",
      "layer1.2.bn1.bias \t torch.Size([64])\n",
      "layer1.2.bn1.running_mean \t torch.Size([64])\n",
      "layer1.2.bn1.running_var \t torch.Size([64])\n",
      "layer1.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.2.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.2.bn2.weight \t torch.Size([64])\n",
      "layer1.2.bn2.bias \t torch.Size([64])\n",
      "layer1.2.bn2.running_mean \t torch.Size([64])\n",
      "layer1.2.bn2.running_var \t torch.Size([64])\n",
      "layer1.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer1.2.conv3.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.2.bn3.weight \t torch.Size([256])\n",
      "layer1.2.bn3.bias \t torch.Size([256])\n",
      "layer1.2.bn3.running_mean \t torch.Size([256])\n",
      "layer1.2.bn3.running_var \t torch.Size([256])\n",
      "layer1.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.conv1.weight \t torch.Size([128, 256, 1, 1])\n",
      "layer2.0.bn1.weight \t torch.Size([128])\n",
      "layer2.0.bn1.bias \t torch.Size([128])\n",
      "layer2.0.bn1.running_mean \t torch.Size([128])\n",
      "layer2.0.bn1.running_var \t torch.Size([128])\n",
      "layer2.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.0.bn2.weight \t torch.Size([128])\n",
      "layer2.0.bn2.bias \t torch.Size([128])\n",
      "layer2.0.bn2.running_mean \t torch.Size([128])\n",
      "layer2.0.bn2.running_var \t torch.Size([128])\n",
      "layer2.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.0.bn3.weight \t torch.Size([512])\n",
      "layer2.0.bn3.bias \t torch.Size([512])\n",
      "layer2.0.bn3.running_mean \t torch.Size([512])\n",
      "layer2.0.bn3.running_var \t torch.Size([512])\n",
      "layer2.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "layer2.0.downsample.1.weight \t torch.Size([512])\n",
      "layer2.0.downsample.1.bias \t torch.Size([512])\n",
      "layer2.0.downsample.1.running_mean \t torch.Size([512])\n",
      "layer2.0.downsample.1.running_var \t torch.Size([512])\n",
      "layer2.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer2.1.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.1.bn1.weight \t torch.Size([128])\n",
      "layer2.1.bn1.bias \t torch.Size([128])\n",
      "layer2.1.bn1.running_mean \t torch.Size([128])\n",
      "layer2.1.bn1.running_var \t torch.Size([128])\n",
      "layer2.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn2.weight \t torch.Size([128])\n",
      "layer2.1.bn2.bias \t torch.Size([128])\n",
      "layer2.1.bn2.running_mean \t torch.Size([128])\n",
      "layer2.1.bn2.running_var \t torch.Size([128])\n",
      "layer2.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.1.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.1.bn3.weight \t torch.Size([512])\n",
      "layer2.1.bn3.bias \t torch.Size([512])\n",
      "layer2.1.bn3.running_mean \t torch.Size([512])\n",
      "layer2.1.bn3.running_var \t torch.Size([512])\n",
      "layer2.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.2.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.2.bn1.weight \t torch.Size([128])\n",
      "layer2.2.bn1.bias \t torch.Size([128])\n",
      "layer2.2.bn1.running_mean \t torch.Size([128])\n",
      "layer2.2.bn1.running_var \t torch.Size([128])\n",
      "layer2.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.2.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.2.bn2.weight \t torch.Size([128])\n",
      "layer2.2.bn2.bias \t torch.Size([128])\n",
      "layer2.2.bn2.running_mean \t torch.Size([128])\n",
      "layer2.2.bn2.running_var \t torch.Size([128])\n",
      "layer2.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.2.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.2.bn3.weight \t torch.Size([512])\n",
      "layer2.2.bn3.bias \t torch.Size([512])\n",
      "layer2.2.bn3.running_mean \t torch.Size([512])\n",
      "layer2.2.bn3.running_var \t torch.Size([512])\n",
      "layer2.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.3.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.3.bn1.weight \t torch.Size([128])\n",
      "layer2.3.bn1.bias \t torch.Size([128])\n",
      "layer2.3.bn1.running_mean \t torch.Size([128])\n",
      "layer2.3.bn1.running_var \t torch.Size([128])\n",
      "layer2.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.3.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.3.bn2.weight \t torch.Size([128])\n",
      "layer2.3.bn2.bias \t torch.Size([128])\n",
      "layer2.3.bn2.running_mean \t torch.Size([128])\n",
      "layer2.3.bn2.running_var \t torch.Size([128])\n",
      "layer2.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.3.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.3.bn3.weight \t torch.Size([512])\n",
      "layer2.3.bn3.bias \t torch.Size([512])\n",
      "layer2.3.bn3.running_mean \t torch.Size([512])\n",
      "layer2.3.bn3.running_var \t torch.Size([512])\n",
      "layer2.3.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.4.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.4.bn1.weight \t torch.Size([128])\n",
      "layer2.4.bn1.bias \t torch.Size([128])\n",
      "layer2.4.bn1.running_mean \t torch.Size([128])\n",
      "layer2.4.bn1.running_var \t torch.Size([128])\n",
      "layer2.4.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.4.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.4.bn2.weight \t torch.Size([128])\n",
      "layer2.4.bn2.bias \t torch.Size([128])\n",
      "layer2.4.bn2.running_mean \t torch.Size([128])\n",
      "layer2.4.bn2.running_var \t torch.Size([128])\n",
      "layer2.4.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.4.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.4.bn3.weight \t torch.Size([512])\n",
      "layer2.4.bn3.bias \t torch.Size([512])\n",
      "layer2.4.bn3.running_mean \t torch.Size([512])\n",
      "layer2.4.bn3.running_var \t torch.Size([512])\n",
      "layer2.4.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.5.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.5.bn1.weight \t torch.Size([128])\n",
      "layer2.5.bn1.bias \t torch.Size([128])\n",
      "layer2.5.bn1.running_mean \t torch.Size([128])\n",
      "layer2.5.bn1.running_var \t torch.Size([128])\n",
      "layer2.5.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.5.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.5.bn2.weight \t torch.Size([128])\n",
      "layer2.5.bn2.bias \t torch.Size([128])\n",
      "layer2.5.bn2.running_mean \t torch.Size([128])\n",
      "layer2.5.bn2.running_var \t torch.Size([128])\n",
      "layer2.5.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.5.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.5.bn3.weight \t torch.Size([512])\n",
      "layer2.5.bn3.bias \t torch.Size([512])\n",
      "layer2.5.bn3.running_mean \t torch.Size([512])\n",
      "layer2.5.bn3.running_var \t torch.Size([512])\n",
      "layer2.5.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.6.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.6.bn1.weight \t torch.Size([128])\n",
      "layer2.6.bn1.bias \t torch.Size([128])\n",
      "layer2.6.bn1.running_mean \t torch.Size([128])\n",
      "layer2.6.bn1.running_var \t torch.Size([128])\n",
      "layer2.6.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.6.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.6.bn2.weight \t torch.Size([128])\n",
      "layer2.6.bn2.bias \t torch.Size([128])\n",
      "layer2.6.bn2.running_mean \t torch.Size([128])\n",
      "layer2.6.bn2.running_var \t torch.Size([128])\n",
      "layer2.6.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.6.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.6.bn3.weight \t torch.Size([512])\n",
      "layer2.6.bn3.bias \t torch.Size([512])\n",
      "layer2.6.bn3.running_mean \t torch.Size([512])\n",
      "layer2.6.bn3.running_var \t torch.Size([512])\n",
      "layer2.6.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.7.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.7.bn1.weight \t torch.Size([128])\n",
      "layer2.7.bn1.bias \t torch.Size([128])\n",
      "layer2.7.bn1.running_mean \t torch.Size([128])\n",
      "layer2.7.bn1.running_var \t torch.Size([128])\n",
      "layer2.7.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.7.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.7.bn2.weight \t torch.Size([128])\n",
      "layer2.7.bn2.bias \t torch.Size([128])\n",
      "layer2.7.bn2.running_mean \t torch.Size([128])\n",
      "layer2.7.bn2.running_var \t torch.Size([128])\n",
      "layer2.7.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.7.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.7.bn3.weight \t torch.Size([512])\n",
      "layer2.7.bn3.bias \t torch.Size([512])\n",
      "layer2.7.bn3.running_mean \t torch.Size([512])\n",
      "layer2.7.bn3.running_var \t torch.Size([512])\n",
      "layer2.7.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.conv1.weight \t torch.Size([256, 512, 1, 1])\n",
      "layer3.0.bn1.weight \t torch.Size([256])\n",
      "layer3.0.bn1.bias \t torch.Size([256])\n",
      "layer3.0.bn1.running_mean \t torch.Size([256])\n",
      "layer3.0.bn1.running_var \t torch.Size([256])\n",
      "layer3.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.0.bn2.weight \t torch.Size([256])\n",
      "layer3.0.bn2.bias \t torch.Size([256])\n",
      "layer3.0.bn2.running_mean \t torch.Size([256])\n",
      "layer3.0.bn2.running_var \t torch.Size([256])\n",
      "layer3.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.0.bn3.weight \t torch.Size([1024])\n",
      "layer3.0.bn3.bias \t torch.Size([1024])\n",
      "layer3.0.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.0.bn3.running_var \t torch.Size([1024])\n",
      "layer3.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.downsample.0.weight \t torch.Size([1024, 512, 1, 1])\n",
      "layer3.0.downsample.1.weight \t torch.Size([1024])\n",
      "layer3.0.downsample.1.bias \t torch.Size([1024])\n",
      "layer3.0.downsample.1.running_mean \t torch.Size([1024])\n",
      "layer3.0.downsample.1.running_var \t torch.Size([1024])\n",
      "layer3.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer3.1.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.1.bn1.weight \t torch.Size([256])\n",
      "layer3.1.bn1.bias \t torch.Size([256])\n",
      "layer3.1.bn1.running_mean \t torch.Size([256])\n",
      "layer3.1.bn1.running_var \t torch.Size([256])\n",
      "layer3.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn2.weight \t torch.Size([256])\n",
      "layer3.1.bn2.bias \t torch.Size([256])\n",
      "layer3.1.bn2.running_mean \t torch.Size([256])\n",
      "layer3.1.bn2.running_var \t torch.Size([256])\n",
      "layer3.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.1.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.1.bn3.weight \t torch.Size([1024])\n",
      "layer3.1.bn3.bias \t torch.Size([1024])\n",
      "layer3.1.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.1.bn3.running_var \t torch.Size([1024])\n",
      "layer3.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.2.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.2.bn1.weight \t torch.Size([256])\n",
      "layer3.2.bn1.bias \t torch.Size([256])\n",
      "layer3.2.bn1.running_mean \t torch.Size([256])\n",
      "layer3.2.bn1.running_var \t torch.Size([256])\n",
      "layer3.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.2.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.2.bn2.weight \t torch.Size([256])\n",
      "layer3.2.bn2.bias \t torch.Size([256])\n",
      "layer3.2.bn2.running_mean \t torch.Size([256])\n",
      "layer3.2.bn2.running_var \t torch.Size([256])\n",
      "layer3.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.2.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.2.bn3.weight \t torch.Size([1024])\n",
      "layer3.2.bn3.bias \t torch.Size([1024])\n",
      "layer3.2.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.2.bn3.running_var \t torch.Size([1024])\n",
      "layer3.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.3.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.3.bn1.weight \t torch.Size([256])\n",
      "layer3.3.bn1.bias \t torch.Size([256])\n",
      "layer3.3.bn1.running_mean \t torch.Size([256])\n",
      "layer3.3.bn1.running_var \t torch.Size([256])\n",
      "layer3.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.3.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.3.bn2.weight \t torch.Size([256])\n",
      "layer3.3.bn2.bias \t torch.Size([256])\n",
      "layer3.3.bn2.running_mean \t torch.Size([256])\n",
      "layer3.3.bn2.running_var \t torch.Size([256])\n",
      "layer3.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.3.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.3.bn3.weight \t torch.Size([1024])\n",
      "layer3.3.bn3.bias \t torch.Size([1024])\n",
      "layer3.3.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.3.bn3.running_var \t torch.Size([1024])\n",
      "layer3.3.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.4.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.4.bn1.weight \t torch.Size([256])\n",
      "layer3.4.bn1.bias \t torch.Size([256])\n",
      "layer3.4.bn1.running_mean \t torch.Size([256])\n",
      "layer3.4.bn1.running_var \t torch.Size([256])\n",
      "layer3.4.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.4.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.4.bn2.weight \t torch.Size([256])\n",
      "layer3.4.bn2.bias \t torch.Size([256])\n",
      "layer3.4.bn2.running_mean \t torch.Size([256])\n",
      "layer3.4.bn2.running_var \t torch.Size([256])\n",
      "layer3.4.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.4.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.4.bn3.weight \t torch.Size([1024])\n",
      "layer3.4.bn3.bias \t torch.Size([1024])\n",
      "layer3.4.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.4.bn3.running_var \t torch.Size([1024])\n",
      "layer3.4.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.5.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.5.bn1.weight \t torch.Size([256])\n",
      "layer3.5.bn1.bias \t torch.Size([256])\n",
      "layer3.5.bn1.running_mean \t torch.Size([256])\n",
      "layer3.5.bn1.running_var \t torch.Size([256])\n",
      "layer3.5.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.5.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.5.bn2.weight \t torch.Size([256])\n",
      "layer3.5.bn2.bias \t torch.Size([256])\n",
      "layer3.5.bn2.running_mean \t torch.Size([256])\n",
      "layer3.5.bn2.running_var \t torch.Size([256])\n",
      "layer3.5.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.5.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.5.bn3.weight \t torch.Size([1024])\n",
      "layer3.5.bn3.bias \t torch.Size([1024])\n",
      "layer3.5.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.5.bn3.running_var \t torch.Size([1024])\n",
      "layer3.5.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.6.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.6.bn1.weight \t torch.Size([256])\n",
      "layer3.6.bn1.bias \t torch.Size([256])\n",
      "layer3.6.bn1.running_mean \t torch.Size([256])\n",
      "layer3.6.bn1.running_var \t torch.Size([256])\n",
      "layer3.6.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.6.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.6.bn2.weight \t torch.Size([256])\n",
      "layer3.6.bn2.bias \t torch.Size([256])\n",
      "layer3.6.bn2.running_mean \t torch.Size([256])\n",
      "layer3.6.bn2.running_var \t torch.Size([256])\n",
      "layer3.6.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.6.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.6.bn3.weight \t torch.Size([1024])\n",
      "layer3.6.bn3.bias \t torch.Size([1024])\n",
      "layer3.6.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.6.bn3.running_var \t torch.Size([1024])\n",
      "layer3.6.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.7.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.7.bn1.weight \t torch.Size([256])\n",
      "layer3.7.bn1.bias \t torch.Size([256])\n",
      "layer3.7.bn1.running_mean \t torch.Size([256])\n",
      "layer3.7.bn1.running_var \t torch.Size([256])\n",
      "layer3.7.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.7.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.7.bn2.weight \t torch.Size([256])\n",
      "layer3.7.bn2.bias \t torch.Size([256])\n",
      "layer3.7.bn2.running_mean \t torch.Size([256])\n",
      "layer3.7.bn2.running_var \t torch.Size([256])\n",
      "layer3.7.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.7.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.7.bn3.weight \t torch.Size([1024])\n",
      "layer3.7.bn3.bias \t torch.Size([1024])\n",
      "layer3.7.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.7.bn3.running_var \t torch.Size([1024])\n",
      "layer3.7.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.8.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.8.bn1.weight \t torch.Size([256])\n",
      "layer3.8.bn1.bias \t torch.Size([256])\n",
      "layer3.8.bn1.running_mean \t torch.Size([256])\n",
      "layer3.8.bn1.running_var \t torch.Size([256])\n",
      "layer3.8.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.8.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.8.bn2.weight \t torch.Size([256])\n",
      "layer3.8.bn2.bias \t torch.Size([256])\n",
      "layer3.8.bn2.running_mean \t torch.Size([256])\n",
      "layer3.8.bn2.running_var \t torch.Size([256])\n",
      "layer3.8.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.8.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.8.bn3.weight \t torch.Size([1024])\n",
      "layer3.8.bn3.bias \t torch.Size([1024])\n",
      "layer3.8.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.8.bn3.running_var \t torch.Size([1024])\n",
      "layer3.8.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.9.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.9.bn1.weight \t torch.Size([256])\n",
      "layer3.9.bn1.bias \t torch.Size([256])\n",
      "layer3.9.bn1.running_mean \t torch.Size([256])\n",
      "layer3.9.bn1.running_var \t torch.Size([256])\n",
      "layer3.9.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.9.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.9.bn2.weight \t torch.Size([256])\n",
      "layer3.9.bn2.bias \t torch.Size([256])\n",
      "layer3.9.bn2.running_mean \t torch.Size([256])\n",
      "layer3.9.bn2.running_var \t torch.Size([256])\n",
      "layer3.9.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.9.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.9.bn3.weight \t torch.Size([1024])\n",
      "layer3.9.bn3.bias \t torch.Size([1024])\n",
      "layer3.9.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.9.bn3.running_var \t torch.Size([1024])\n",
      "layer3.9.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.10.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.10.bn1.weight \t torch.Size([256])\n",
      "layer3.10.bn1.bias \t torch.Size([256])\n",
      "layer3.10.bn1.running_mean \t torch.Size([256])\n",
      "layer3.10.bn1.running_var \t torch.Size([256])\n",
      "layer3.10.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.10.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.10.bn2.weight \t torch.Size([256])\n",
      "layer3.10.bn2.bias \t torch.Size([256])\n",
      "layer3.10.bn2.running_mean \t torch.Size([256])\n",
      "layer3.10.bn2.running_var \t torch.Size([256])\n",
      "layer3.10.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.10.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.10.bn3.weight \t torch.Size([1024])\n",
      "layer3.10.bn3.bias \t torch.Size([1024])\n",
      "layer3.10.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.10.bn3.running_var \t torch.Size([1024])\n",
      "layer3.10.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.11.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.11.bn1.weight \t torch.Size([256])\n",
      "layer3.11.bn1.bias \t torch.Size([256])\n",
      "layer3.11.bn1.running_mean \t torch.Size([256])\n",
      "layer3.11.bn1.running_var \t torch.Size([256])\n",
      "layer3.11.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.11.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.11.bn2.weight \t torch.Size([256])\n",
      "layer3.11.bn2.bias \t torch.Size([256])\n",
      "layer3.11.bn2.running_mean \t torch.Size([256])\n",
      "layer3.11.bn2.running_var \t torch.Size([256])\n",
      "layer3.11.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.11.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.11.bn3.weight \t torch.Size([1024])\n",
      "layer3.11.bn3.bias \t torch.Size([1024])\n",
      "layer3.11.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.11.bn3.running_var \t torch.Size([1024])\n",
      "layer3.11.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.12.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.12.bn1.weight \t torch.Size([256])\n",
      "layer3.12.bn1.bias \t torch.Size([256])\n",
      "layer3.12.bn1.running_mean \t torch.Size([256])\n",
      "layer3.12.bn1.running_var \t torch.Size([256])\n",
      "layer3.12.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.12.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.12.bn2.weight \t torch.Size([256])\n",
      "layer3.12.bn2.bias \t torch.Size([256])\n",
      "layer3.12.bn2.running_mean \t torch.Size([256])\n",
      "layer3.12.bn2.running_var \t torch.Size([256])\n",
      "layer3.12.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.12.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.12.bn3.weight \t torch.Size([1024])\n",
      "layer3.12.bn3.bias \t torch.Size([1024])\n",
      "layer3.12.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.12.bn3.running_var \t torch.Size([1024])\n",
      "layer3.12.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.13.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.13.bn1.weight \t torch.Size([256])\n",
      "layer3.13.bn1.bias \t torch.Size([256])\n",
      "layer3.13.bn1.running_mean \t torch.Size([256])\n",
      "layer3.13.bn1.running_var \t torch.Size([256])\n",
      "layer3.13.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.13.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.13.bn2.weight \t torch.Size([256])\n",
      "layer3.13.bn2.bias \t torch.Size([256])\n",
      "layer3.13.bn2.running_mean \t torch.Size([256])\n",
      "layer3.13.bn2.running_var \t torch.Size([256])\n",
      "layer3.13.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.13.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.13.bn3.weight \t torch.Size([1024])\n",
      "layer3.13.bn3.bias \t torch.Size([1024])\n",
      "layer3.13.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.13.bn3.running_var \t torch.Size([1024])\n",
      "layer3.13.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.14.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.14.bn1.weight \t torch.Size([256])\n",
      "layer3.14.bn1.bias \t torch.Size([256])\n",
      "layer3.14.bn1.running_mean \t torch.Size([256])\n",
      "layer3.14.bn1.running_var \t torch.Size([256])\n",
      "layer3.14.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.14.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.14.bn2.weight \t torch.Size([256])\n",
      "layer3.14.bn2.bias \t torch.Size([256])\n",
      "layer3.14.bn2.running_mean \t torch.Size([256])\n",
      "layer3.14.bn2.running_var \t torch.Size([256])\n",
      "layer3.14.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.14.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.14.bn3.weight \t torch.Size([1024])\n",
      "layer3.14.bn3.bias \t torch.Size([1024])\n",
      "layer3.14.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.14.bn3.running_var \t torch.Size([1024])\n",
      "layer3.14.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.15.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.15.bn1.weight \t torch.Size([256])\n",
      "layer3.15.bn1.bias \t torch.Size([256])\n",
      "layer3.15.bn1.running_mean \t torch.Size([256])\n",
      "layer3.15.bn1.running_var \t torch.Size([256])\n",
      "layer3.15.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.15.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.15.bn2.weight \t torch.Size([256])\n",
      "layer3.15.bn2.bias \t torch.Size([256])\n",
      "layer3.15.bn2.running_mean \t torch.Size([256])\n",
      "layer3.15.bn2.running_var \t torch.Size([256])\n",
      "layer3.15.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.15.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.15.bn3.weight \t torch.Size([1024])\n",
      "layer3.15.bn3.bias \t torch.Size([1024])\n",
      "layer3.15.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.15.bn3.running_var \t torch.Size([1024])\n",
      "layer3.15.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.16.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.16.bn1.weight \t torch.Size([256])\n",
      "layer3.16.bn1.bias \t torch.Size([256])\n",
      "layer3.16.bn1.running_mean \t torch.Size([256])\n",
      "layer3.16.bn1.running_var \t torch.Size([256])\n",
      "layer3.16.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.16.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.16.bn2.weight \t torch.Size([256])\n",
      "layer3.16.bn2.bias \t torch.Size([256])\n",
      "layer3.16.bn2.running_mean \t torch.Size([256])\n",
      "layer3.16.bn2.running_var \t torch.Size([256])\n",
      "layer3.16.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.16.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.16.bn3.weight \t torch.Size([1024])\n",
      "layer3.16.bn3.bias \t torch.Size([1024])\n",
      "layer3.16.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.16.bn3.running_var \t torch.Size([1024])\n",
      "layer3.16.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.17.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.17.bn1.weight \t torch.Size([256])\n",
      "layer3.17.bn1.bias \t torch.Size([256])\n",
      "layer3.17.bn1.running_mean \t torch.Size([256])\n",
      "layer3.17.bn1.running_var \t torch.Size([256])\n",
      "layer3.17.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.17.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.17.bn2.weight \t torch.Size([256])\n",
      "layer3.17.bn2.bias \t torch.Size([256])\n",
      "layer3.17.bn2.running_mean \t torch.Size([256])\n",
      "layer3.17.bn2.running_var \t torch.Size([256])\n",
      "layer3.17.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.17.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.17.bn3.weight \t torch.Size([1024])\n",
      "layer3.17.bn3.bias \t torch.Size([1024])\n",
      "layer3.17.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.17.bn3.running_var \t torch.Size([1024])\n",
      "layer3.17.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.18.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.18.bn1.weight \t torch.Size([256])\n",
      "layer3.18.bn1.bias \t torch.Size([256])\n",
      "layer3.18.bn1.running_mean \t torch.Size([256])\n",
      "layer3.18.bn1.running_var \t torch.Size([256])\n",
      "layer3.18.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.18.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.18.bn2.weight \t torch.Size([256])\n",
      "layer3.18.bn2.bias \t torch.Size([256])\n",
      "layer3.18.bn2.running_mean \t torch.Size([256])\n",
      "layer3.18.bn2.running_var \t torch.Size([256])\n",
      "layer3.18.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.18.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.18.bn3.weight \t torch.Size([1024])\n",
      "layer3.18.bn3.bias \t torch.Size([1024])\n",
      "layer3.18.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.18.bn3.running_var \t torch.Size([1024])\n",
      "layer3.18.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.19.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.19.bn1.weight \t torch.Size([256])\n",
      "layer3.19.bn1.bias \t torch.Size([256])\n",
      "layer3.19.bn1.running_mean \t torch.Size([256])\n",
      "layer3.19.bn1.running_var \t torch.Size([256])\n",
      "layer3.19.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.19.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.19.bn2.weight \t torch.Size([256])\n",
      "layer3.19.bn2.bias \t torch.Size([256])\n",
      "layer3.19.bn2.running_mean \t torch.Size([256])\n",
      "layer3.19.bn2.running_var \t torch.Size([256])\n",
      "layer3.19.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.19.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.19.bn3.weight \t torch.Size([1024])\n",
      "layer3.19.bn3.bias \t torch.Size([1024])\n",
      "layer3.19.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.19.bn3.running_var \t torch.Size([1024])\n",
      "layer3.19.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.20.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.20.bn1.weight \t torch.Size([256])\n",
      "layer3.20.bn1.bias \t torch.Size([256])\n",
      "layer3.20.bn1.running_mean \t torch.Size([256])\n",
      "layer3.20.bn1.running_var \t torch.Size([256])\n",
      "layer3.20.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.20.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.20.bn2.weight \t torch.Size([256])\n",
      "layer3.20.bn2.bias \t torch.Size([256])\n",
      "layer3.20.bn2.running_mean \t torch.Size([256])\n",
      "layer3.20.bn2.running_var \t torch.Size([256])\n",
      "layer3.20.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.20.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.20.bn3.weight \t torch.Size([1024])\n",
      "layer3.20.bn3.bias \t torch.Size([1024])\n",
      "layer3.20.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.20.bn3.running_var \t torch.Size([1024])\n",
      "layer3.20.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.21.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.21.bn1.weight \t torch.Size([256])\n",
      "layer3.21.bn1.bias \t torch.Size([256])\n",
      "layer3.21.bn1.running_mean \t torch.Size([256])\n",
      "layer3.21.bn1.running_var \t torch.Size([256])\n",
      "layer3.21.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.21.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.21.bn2.weight \t torch.Size([256])\n",
      "layer3.21.bn2.bias \t torch.Size([256])\n",
      "layer3.21.bn2.running_mean \t torch.Size([256])\n",
      "layer3.21.bn2.running_var \t torch.Size([256])\n",
      "layer3.21.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.21.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.21.bn3.weight \t torch.Size([1024])\n",
      "layer3.21.bn3.bias \t torch.Size([1024])\n",
      "layer3.21.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.21.bn3.running_var \t torch.Size([1024])\n",
      "layer3.21.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.22.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.22.bn1.weight \t torch.Size([256])\n",
      "layer3.22.bn1.bias \t torch.Size([256])\n",
      "layer3.22.bn1.running_mean \t torch.Size([256])\n",
      "layer3.22.bn1.running_var \t torch.Size([256])\n",
      "layer3.22.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.22.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.22.bn2.weight \t torch.Size([256])\n",
      "layer3.22.bn2.bias \t torch.Size([256])\n",
      "layer3.22.bn2.running_mean \t torch.Size([256])\n",
      "layer3.22.bn2.running_var \t torch.Size([256])\n",
      "layer3.22.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.22.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.22.bn3.weight \t torch.Size([1024])\n",
      "layer3.22.bn3.bias \t torch.Size([1024])\n",
      "layer3.22.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.22.bn3.running_var \t torch.Size([1024])\n",
      "layer3.22.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.23.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.23.bn1.weight \t torch.Size([256])\n",
      "layer3.23.bn1.bias \t torch.Size([256])\n",
      "layer3.23.bn1.running_mean \t torch.Size([256])\n",
      "layer3.23.bn1.running_var \t torch.Size([256])\n",
      "layer3.23.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.23.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.23.bn2.weight \t torch.Size([256])\n",
      "layer3.23.bn2.bias \t torch.Size([256])\n",
      "layer3.23.bn2.running_mean \t torch.Size([256])\n",
      "layer3.23.bn2.running_var \t torch.Size([256])\n",
      "layer3.23.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.23.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.23.bn3.weight \t torch.Size([1024])\n",
      "layer3.23.bn3.bias \t torch.Size([1024])\n",
      "layer3.23.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.23.bn3.running_var \t torch.Size([1024])\n",
      "layer3.23.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.24.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.24.bn1.weight \t torch.Size([256])\n",
      "layer3.24.bn1.bias \t torch.Size([256])\n",
      "layer3.24.bn1.running_mean \t torch.Size([256])\n",
      "layer3.24.bn1.running_var \t torch.Size([256])\n",
      "layer3.24.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.24.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.24.bn2.weight \t torch.Size([256])\n",
      "layer3.24.bn2.bias \t torch.Size([256])\n",
      "layer3.24.bn2.running_mean \t torch.Size([256])\n",
      "layer3.24.bn2.running_var \t torch.Size([256])\n",
      "layer3.24.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.24.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.24.bn3.weight \t torch.Size([1024])\n",
      "layer3.24.bn3.bias \t torch.Size([1024])\n",
      "layer3.24.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.24.bn3.running_var \t torch.Size([1024])\n",
      "layer3.24.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.25.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.25.bn1.weight \t torch.Size([256])\n",
      "layer3.25.bn1.bias \t torch.Size([256])\n",
      "layer3.25.bn1.running_mean \t torch.Size([256])\n",
      "layer3.25.bn1.running_var \t torch.Size([256])\n",
      "layer3.25.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.25.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.25.bn2.weight \t torch.Size([256])\n",
      "layer3.25.bn2.bias \t torch.Size([256])\n",
      "layer3.25.bn2.running_mean \t torch.Size([256])\n",
      "layer3.25.bn2.running_var \t torch.Size([256])\n",
      "layer3.25.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.25.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.25.bn3.weight \t torch.Size([1024])\n",
      "layer3.25.bn3.bias \t torch.Size([1024])\n",
      "layer3.25.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.25.bn3.running_var \t torch.Size([1024])\n",
      "layer3.25.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.26.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.26.bn1.weight \t torch.Size([256])\n",
      "layer3.26.bn1.bias \t torch.Size([256])\n",
      "layer3.26.bn1.running_mean \t torch.Size([256])\n",
      "layer3.26.bn1.running_var \t torch.Size([256])\n",
      "layer3.26.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.26.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.26.bn2.weight \t torch.Size([256])\n",
      "layer3.26.bn2.bias \t torch.Size([256])\n",
      "layer3.26.bn2.running_mean \t torch.Size([256])\n",
      "layer3.26.bn2.running_var \t torch.Size([256])\n",
      "layer3.26.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.26.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.26.bn3.weight \t torch.Size([1024])\n",
      "layer3.26.bn3.bias \t torch.Size([1024])\n",
      "layer3.26.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.26.bn3.running_var \t torch.Size([1024])\n",
      "layer3.26.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.27.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.27.bn1.weight \t torch.Size([256])\n",
      "layer3.27.bn1.bias \t torch.Size([256])\n",
      "layer3.27.bn1.running_mean \t torch.Size([256])\n",
      "layer3.27.bn1.running_var \t torch.Size([256])\n",
      "layer3.27.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.27.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.27.bn2.weight \t torch.Size([256])\n",
      "layer3.27.bn2.bias \t torch.Size([256])\n",
      "layer3.27.bn2.running_mean \t torch.Size([256])\n",
      "layer3.27.bn2.running_var \t torch.Size([256])\n",
      "layer3.27.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.27.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.27.bn3.weight \t torch.Size([1024])\n",
      "layer3.27.bn3.bias \t torch.Size([1024])\n",
      "layer3.27.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.27.bn3.running_var \t torch.Size([1024])\n",
      "layer3.27.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.28.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.28.bn1.weight \t torch.Size([256])\n",
      "layer3.28.bn1.bias \t torch.Size([256])\n",
      "layer3.28.bn1.running_mean \t torch.Size([256])\n",
      "layer3.28.bn1.running_var \t torch.Size([256])\n",
      "layer3.28.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.28.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.28.bn2.weight \t torch.Size([256])\n",
      "layer3.28.bn2.bias \t torch.Size([256])\n",
      "layer3.28.bn2.running_mean \t torch.Size([256])\n",
      "layer3.28.bn2.running_var \t torch.Size([256])\n",
      "layer3.28.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.28.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.28.bn3.weight \t torch.Size([1024])\n",
      "layer3.28.bn3.bias \t torch.Size([1024])\n",
      "layer3.28.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.28.bn3.running_var \t torch.Size([1024])\n",
      "layer3.28.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.29.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.29.bn1.weight \t torch.Size([256])\n",
      "layer3.29.bn1.bias \t torch.Size([256])\n",
      "layer3.29.bn1.running_mean \t torch.Size([256])\n",
      "layer3.29.bn1.running_var \t torch.Size([256])\n",
      "layer3.29.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.29.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.29.bn2.weight \t torch.Size([256])\n",
      "layer3.29.bn2.bias \t torch.Size([256])\n",
      "layer3.29.bn2.running_mean \t torch.Size([256])\n",
      "layer3.29.bn2.running_var \t torch.Size([256])\n",
      "layer3.29.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.29.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.29.bn3.weight \t torch.Size([1024])\n",
      "layer3.29.bn3.bias \t torch.Size([1024])\n",
      "layer3.29.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.29.bn3.running_var \t torch.Size([1024])\n",
      "layer3.29.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.30.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.30.bn1.weight \t torch.Size([256])\n",
      "layer3.30.bn1.bias \t torch.Size([256])\n",
      "layer3.30.bn1.running_mean \t torch.Size([256])\n",
      "layer3.30.bn1.running_var \t torch.Size([256])\n",
      "layer3.30.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.30.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.30.bn2.weight \t torch.Size([256])\n",
      "layer3.30.bn2.bias \t torch.Size([256])\n",
      "layer3.30.bn2.running_mean \t torch.Size([256])\n",
      "layer3.30.bn2.running_var \t torch.Size([256])\n",
      "layer3.30.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.30.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.30.bn3.weight \t torch.Size([1024])\n",
      "layer3.30.bn3.bias \t torch.Size([1024])\n",
      "layer3.30.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.30.bn3.running_var \t torch.Size([1024])\n",
      "layer3.30.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.31.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.31.bn1.weight \t torch.Size([256])\n",
      "layer3.31.bn1.bias \t torch.Size([256])\n",
      "layer3.31.bn1.running_mean \t torch.Size([256])\n",
      "layer3.31.bn1.running_var \t torch.Size([256])\n",
      "layer3.31.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.31.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.31.bn2.weight \t torch.Size([256])\n",
      "layer3.31.bn2.bias \t torch.Size([256])\n",
      "layer3.31.bn2.running_mean \t torch.Size([256])\n",
      "layer3.31.bn2.running_var \t torch.Size([256])\n",
      "layer3.31.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.31.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.31.bn3.weight \t torch.Size([1024])\n",
      "layer3.31.bn3.bias \t torch.Size([1024])\n",
      "layer3.31.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.31.bn3.running_var \t torch.Size([1024])\n",
      "layer3.31.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.32.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.32.bn1.weight \t torch.Size([256])\n",
      "layer3.32.bn1.bias \t torch.Size([256])\n",
      "layer3.32.bn1.running_mean \t torch.Size([256])\n",
      "layer3.32.bn1.running_var \t torch.Size([256])\n",
      "layer3.32.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.32.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.32.bn2.weight \t torch.Size([256])\n",
      "layer3.32.bn2.bias \t torch.Size([256])\n",
      "layer3.32.bn2.running_mean \t torch.Size([256])\n",
      "layer3.32.bn2.running_var \t torch.Size([256])\n",
      "layer3.32.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.32.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.32.bn3.weight \t torch.Size([1024])\n",
      "layer3.32.bn3.bias \t torch.Size([1024])\n",
      "layer3.32.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.32.bn3.running_var \t torch.Size([1024])\n",
      "layer3.32.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.33.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.33.bn1.weight \t torch.Size([256])\n",
      "layer3.33.bn1.bias \t torch.Size([256])\n",
      "layer3.33.bn1.running_mean \t torch.Size([256])\n",
      "layer3.33.bn1.running_var \t torch.Size([256])\n",
      "layer3.33.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.33.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.33.bn2.weight \t torch.Size([256])\n",
      "layer3.33.bn2.bias \t torch.Size([256])\n",
      "layer3.33.bn2.running_mean \t torch.Size([256])\n",
      "layer3.33.bn2.running_var \t torch.Size([256])\n",
      "layer3.33.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.33.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.33.bn3.weight \t torch.Size([1024])\n",
      "layer3.33.bn3.bias \t torch.Size([1024])\n",
      "layer3.33.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.33.bn3.running_var \t torch.Size([1024])\n",
      "layer3.33.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.34.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.34.bn1.weight \t torch.Size([256])\n",
      "layer3.34.bn1.bias \t torch.Size([256])\n",
      "layer3.34.bn1.running_mean \t torch.Size([256])\n",
      "layer3.34.bn1.running_var \t torch.Size([256])\n",
      "layer3.34.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.34.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.34.bn2.weight \t torch.Size([256])\n",
      "layer3.34.bn2.bias \t torch.Size([256])\n",
      "layer3.34.bn2.running_mean \t torch.Size([256])\n",
      "layer3.34.bn2.running_var \t torch.Size([256])\n",
      "layer3.34.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.34.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.34.bn3.weight \t torch.Size([1024])\n",
      "layer3.34.bn3.bias \t torch.Size([1024])\n",
      "layer3.34.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.34.bn3.running_var \t torch.Size([1024])\n",
      "layer3.34.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.35.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.35.bn1.weight \t torch.Size([256])\n",
      "layer3.35.bn1.bias \t torch.Size([256])\n",
      "layer3.35.bn1.running_mean \t torch.Size([256])\n",
      "layer3.35.bn1.running_var \t torch.Size([256])\n",
      "layer3.35.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.35.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.35.bn2.weight \t torch.Size([256])\n",
      "layer3.35.bn2.bias \t torch.Size([256])\n",
      "layer3.35.bn2.running_mean \t torch.Size([256])\n",
      "layer3.35.bn2.running_var \t torch.Size([256])\n",
      "layer3.35.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.35.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.35.bn3.weight \t torch.Size([1024])\n",
      "layer3.35.bn3.bias \t torch.Size([1024])\n",
      "layer3.35.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.35.bn3.running_var \t torch.Size([1024])\n",
      "layer3.35.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.conv1.weight \t torch.Size([512, 1024, 1, 1])\n",
      "layer4.0.bn1.weight \t torch.Size([512])\n",
      "layer4.0.bn1.bias \t torch.Size([512])\n",
      "layer4.0.bn1.running_mean \t torch.Size([512])\n",
      "layer4.0.bn1.running_var \t torch.Size([512])\n",
      "layer4.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.0.bn2.weight \t torch.Size([512])\n",
      "layer4.0.bn2.bias \t torch.Size([512])\n",
      "layer4.0.bn2.running_mean \t torch.Size([512])\n",
      "layer4.0.bn2.running_var \t torch.Size([512])\n",
      "layer4.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.conv3.weight \t torch.Size([2048, 512, 1, 1])\n",
      "layer4.0.bn3.weight \t torch.Size([2048])\n",
      "layer4.0.bn3.bias \t torch.Size([2048])\n",
      "layer4.0.bn3.running_mean \t torch.Size([2048])\n",
      "layer4.0.bn3.running_var \t torch.Size([2048])\n",
      "layer4.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.downsample.0.weight \t torch.Size([2048, 1024, 1, 1])\n",
      "layer4.0.downsample.1.weight \t torch.Size([2048])\n",
      "layer4.0.downsample.1.bias \t torch.Size([2048])\n",
      "layer4.0.downsample.1.running_mean \t torch.Size([2048])\n",
      "layer4.0.downsample.1.running_var \t torch.Size([2048])\n",
      "layer4.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer4.1.conv1.weight \t torch.Size([512, 2048, 1, 1])\n",
      "layer4.1.bn1.weight \t torch.Size([512])\n",
      "layer4.1.bn1.bias \t torch.Size([512])\n",
      "layer4.1.bn1.running_mean \t torch.Size([512])\n",
      "layer4.1.bn1.running_var \t torch.Size([512])\n",
      "layer4.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer4.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn2.weight \t torch.Size([512])\n",
      "layer4.1.bn2.bias \t torch.Size([512])\n",
      "layer4.1.bn2.running_mean \t torch.Size([512])\n",
      "layer4.1.bn2.running_var \t torch.Size([512])\n",
      "layer4.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer4.1.conv3.weight \t torch.Size([2048, 512, 1, 1])\n",
      "layer4.1.bn3.weight \t torch.Size([2048])\n",
      "layer4.1.bn3.bias \t torch.Size([2048])\n",
      "layer4.1.bn3.running_mean \t torch.Size([2048])\n",
      "layer4.1.bn3.running_var \t torch.Size([2048])\n",
      "layer4.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer4.2.conv1.weight \t torch.Size([512, 2048, 1, 1])\n",
      "layer4.2.bn1.weight \t torch.Size([512])\n",
      "layer4.2.bn1.bias \t torch.Size([512])\n",
      "layer4.2.bn1.running_mean \t torch.Size([512])\n",
      "layer4.2.bn1.running_var \t torch.Size([512])\n",
      "layer4.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer4.2.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.2.bn2.weight \t torch.Size([512])\n",
      "layer4.2.bn2.bias \t torch.Size([512])\n",
      "layer4.2.bn2.running_mean \t torch.Size([512])\n",
      "layer4.2.bn2.running_var \t torch.Size([512])\n",
      "layer4.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer4.2.conv3.weight \t torch.Size([2048, 512, 1, 1])\n",
      "layer4.2.bn3.weight \t torch.Size([2048])\n",
      "layer4.2.bn3.bias \t torch.Size([2048])\n",
      "layer4.2.bn3.running_mean \t torch.Size([2048])\n",
      "layer4.2.bn3.running_var \t torch.Size([2048])\n",
      "layer4.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "fc.0.weight \t torch.Size([256, 2048])\n",
      "fc.0.bias \t torch.Size([256])\n",
      "fc.3.weight \t torch.Size([5, 256])\n",
      "fc.3.bias \t torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0df6ea6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:47.153367Z",
     "iopub.status.busy": "2024-07-06T22:46:47.153075Z",
     "iopub.status.idle": "2024-07-06T22:46:47.158553Z",
     "shell.execute_reply": "2024-07-06T22:46:47.157664Z"
    },
    "papermill": {
     "duration": 0.021968,
     "end_time": "2024-07-06T22:46:47.160540",
     "exception": false,
     "start_time": "2024-07-06T22:46:47.138572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7e261c210510>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e358636",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:47.188903Z",
     "iopub.status.busy": "2024-07-06T22:46:47.188602Z",
     "iopub.status.idle": "2024-07-06T22:46:47.203688Z",
     "shell.execute_reply": "2024-07-06T22:46:47.203014Z"
    },
    "papermill": {
     "duration": 0.031344,
     "end_time": "2024-07-06T22:46:47.205555",
     "exception": false,
     "start_time": "2024-07-06T22:46:47.174211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = Image.open(f'/kaggle/input/cassava-leaf-disease-classification/test_images/2216849948.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98ffae7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:47.234366Z",
     "iopub.status.busy": "2024-07-06T22:46:47.233690Z",
     "iopub.status.idle": "2024-07-06T22:46:47.287938Z",
     "shell.execute_reply": "2024-07-06T22:46:47.287034Z"
    },
    "papermill": {
     "duration": 0.070915,
     "end_time": "2024-07-06T22:46:47.290000",
     "exception": false,
     "start_time": "2024-07-06T22:46:47.219085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = transform_test(image)\n",
    "image = image.to(device)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd5c1750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:47.319174Z",
     "iopub.status.busy": "2024-07-06T22:46:47.318908Z",
     "iopub.status.idle": "2024-07-06T22:46:47.959327Z",
     "shell.execute_reply": "2024-07-06T22:46:47.958314Z"
    },
    "papermill": {
     "duration": 0.657859,
     "end_time": "2024-07-06T22:46:47.961840",
     "exception": false,
     "start_time": "2024-07-06T22:46:47.303981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode(): \n",
    "            # 1. Forward pass\n",
    "            test_pred = model(image.unsqueeze(dim=0))\n",
    "            test_pred = F.softmax(test_pred, dim=1)\n",
    "            test_pred.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebdf19ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:47.990979Z",
     "iopub.status.busy": "2024-07-06T22:46:47.990681Z",
     "iopub.status.idle": "2024-07-06T22:46:48.006185Z",
     "shell.execute_reply": "2024-07-06T22:46:48.005302Z"
    },
    "papermill": {
     "duration": 0.032262,
     "end_time": "2024-07-06T22:46:48.008149",
     "exception": false,
     "start_time": "2024-07-06T22:46:47.975887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  2216849948.jpg      4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb  = pd.read_csv('/kaggle/input/cassava-leaf-disease-classification/sample_submission.csv')\n",
    "submission = sb.copy()\n",
    "submission['label'][0] = test_pred.argmax().cpu().detach().numpy()\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0ff24b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:46:48.036489Z",
     "iopub.status.busy": "2024-07-06T22:46:48.036191Z",
     "iopub.status.idle": "2024-07-06T22:46:48.043177Z",
     "shell.execute_reply": "2024-07-06T22:46:48.042318Z"
    },
    "papermill": {
     "duration": 0.023268,
     "end_time": "2024-07-06T22:46:48.045081",
     "exception": false,
     "start_time": "2024-07-06T22:46:48.021813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe86c0",
   "metadata": {
    "papermill": {
     "duration": 0.013398,
     "end_time": "2024-07-06T22:46:48.072172",
     "exception": false,
     "start_time": "2024-07-06T22:46:48.058774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1718836,
     "sourceId": 13836,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 62921,
     "sourceId": 74918,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20.996999,
   "end_time": "2024-07-06T22:46:49.707509",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-06T22:46:28.710510",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
